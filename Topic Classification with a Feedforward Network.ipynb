{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e92808e3",
   "metadata": {},
   "source": [
    "# Topic Classification with a Feedforward Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27f4b9d",
   "metadata": {},
   "source": [
    "### Data \n",
    "\n",
    "The data you will use for the task is a subset of the [AG News Corpus](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html) and you can find it in the `./data_topic` folder in CSV format:\n",
    "\n",
    "- `data_topic/train.csv`: contains 2,400 news articles, 800 for each class to be used for training.\n",
    "- `data_topic/dev.csv`: contains 150 news articles, 50 for each class to be used for hyperparameter selection and monitoring the training process.\n",
    "- `data_topic/test.csv`: contains 900 news articles, 300 for each class to be used for testing.\n",
    "\n",
    "### Pre-trained Embeddings\n",
    "\n",
    "You can download pre-trained GloVe embeddings trained on Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download) from [here](http://nlp.stanford.edu/data/glove.840B.300d.zip). No need to unzip, the file is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd457697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import random\n",
    "from time import localtime, strftime\n",
    "from scipy.stats import spearmanr,pearsonr\n",
    "import zipfile\n",
    "import gc\n",
    "# fixing random seed for reproducibility\n",
    "random.seed(123)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22caa88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv('train.csv', names=['label', 'feature'])\n",
    "df_dev=pd.read_csv('dev.csv', names=['label', 'feature'])\n",
    "df_test=pd.read_csv('test.csv', names=['label', 'feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "120d7d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list_x = df_train['feature'].tolist()\n",
    "dev_list_x = df_dev['feature'].tolist()\n",
    "test_list_x = df_test['feature'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc178820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text processing is important part of NLP since it can provide a significant improvement in the model performance.\n",
    "# All the punctuations from the text are removed\n",
    "# and then expended stopwords are removed from the text to remove non-content words (meaningless terms)\n",
    "\n",
    "train_list_x = [(re.sub(r'[^\\w\\_\\s]','',str(item))) for item in train_list_x] \n",
    "train_list_x = [(re.sub(\"\\d+\",'',str(item))) for item in train_list_x]\n",
    "\n",
    "dev_list_x = [(re.sub(r'[^\\w\\_\\s]','',str(item))) for item in dev_list_x]\n",
    "dev_list_x = [(re.sub(\"\\d+\",'',str(item))) for item in dev_list_x]\n",
    "\n",
    "test_list_x = [(re.sub(r'[^\\w\\_\\s]','',str(item))) for item in test_list_x]\n",
    "test_list_x = [(re.sub(\"\\d+\",'',str(item))) for item in test_list_x]\n",
    "\n",
    "stop_words = ['a', 'able', 'about', 'above', 'according', 'accordingly', 'across', 'actually', 'after', 'afterwards',\n",
    "              'again', 'against', 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although',\n",
    "              'always', 'am', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone',\n",
    "              'anyway', 'anyways', 'anywhere', 'apart', 'appear', 'appreciate', 'appropriate', 'are', 'around', 'as',\n",
    "              'aside', 'ask', 'asking', 'associated', 'at', 'available', 'away', 'awfully', 'b', 'be', 'became',\n",
    "              'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'believe',\n",
    "              'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'both', 'brief', 'but', 'by', 'c',\n",
    "              'came', 'can', 'cannot', 'cant', 'cause', 'causes', 'certain', 'certainly', 'changes', 'clearly', 'co',\n",
    "              'com', 'come', 'comes', 'concerning', 'consequently', 'consider', 'considering', 'contain', 'containing',\n",
    "              'contains', 'corresponding', 'could', 'course', 'currently', 'd', 'definitely', 'described', 'despite',\n",
    "              'did', 'different', 'do', 'does', 'doing', 'done', 'down', 'downwards', 'during', 'e', 'each', 'edu',\n",
    "              'eg', 'eight', 'either', 'else', 'elsewhere', 'enough', 'entirely', 'especially', 'et', 'etc', 'even',\n",
    "              'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ex', 'exactly', 'example',\n",
    "              'except', 'f', 'far', 'few', 'fifth', 'first', 'five', 'followed', 'following', 'follows', 'for',\n",
    "              'former', 'formerly', 'forth', 'four', 'from', 'further', 'furthermore', 'g', 'get', 'gets', 'getting',\n",
    "              'given', 'gives', 'go', 'goes', 'going', 'gone', 'got', 'gotten', 'greetings', 'h', 'had', 'happens',\n",
    "              'hardly', 'has', 'have', 'having', 'he', 'hello', 'help', 'hence', 'her', 'here', 'hereafter', 'hereby',\n",
    "              'herein', 'hereupon', 'hers', 'herself', 'hi', 'him', 'himself', 'his', 'hither', 'hopefully', 'how',\n",
    "              'howbeit', 'however', 'i', 'ie', 'if', 'ignored', 'immediate', 'in', 'inasmuch', 'inc', 'indeed',\n",
    "              'indicate', 'indicated', 'indicates', 'inner', 'insofar', 'instead', 'into', 'inward', 'is', 'it',\n",
    "              'its', 'itself', 'j', 'just', 'k', 'keep', 'keeps', 'kept', 'know', 'knows', 'known', 'l', 'last',\n",
    "              'lately', 'later', 'latter', 'latterly', 'least', 'less', 'lest', 'let', 'like', 'liked', 'likely',\n",
    "              'little', 'look', 'looking', 'looks', 'ltd', 'm', 'mainly', 'many', 'may', 'maybe', 'me', 'mean',\n",
    "              'meanwhile', 'merely', 'might', 'more', 'moreover', 'most', 'mostly', 'much', 'must', 'my', 'myself',\n",
    "              'n', 'name', 'namely', 'nd', 'near', 'nearly', 'necessary', 'need', 'needs', 'neither', 'never',\n",
    "              'nevertheless', 'new', 'next', 'nine', 'no', 'nobody', 'non', 'none', 'noone', 'nor', 'normally',\n",
    "              'not', 'nothing', 'novel', 'now', 'nowhere', 'o', 'obviously', 'of', 'off', 'often', 'oh', 'ok',\n",
    "              'okay', 'old', 'on', 'once', 'one', 'ones', 'only', 'onto', 'or', 'other', 'others', 'otherwise',\n",
    "              'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'own', 'p', 'particular',\n",
    "              'particularly', 'per', 'perhaps', 'placed', 'please', 'plus', 'possible', 'presumably', 'probably',\n",
    "              'provides', 'q', 'que', 'quite', 'qv', 'r', 'rather', 'rd', 're', 'really', 'reasonably', 'regarding',\n",
    "              'regardless', 'regards', 'relatively', 'respectively', 'right', 's', 'said', 'same', 'saw', 'say',\n",
    "              'saying', 'says', 'second', 'secondly', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen',\n",
    "              'self', 'selves', 'sensible', 'sent', 'serious', 'seriously', 'seven', 'several', 'shall', 'she',\n",
    "              'since', 'six', 'so', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometime', 'sometimes',\n",
    "              'somewhat', 'somewhere', 'soon', 'sorry', 'specified', 'specify', 'specifying', 'still', 'sub', 'such',\n",
    "              'sup', 'sure', 't', 'take', 'taken', 'tell', 'tends', 'th', 'than', 'thank', 'thanks', 'thanx', 'that',\n",
    "              'thats', 'the', '_the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', \n",
    "              'thereby', 'therefore', 'therein', 'theres', 'thereupon', 'these', 'they', 'think', 'third', 'this',\n",
    "              'thorough', 'thoroughly', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to',\n",
    "              'together', 'too', 'took', 'toward', 'towards', 'tried', 'tries', 'truly', 'try', 'trying', 'twice', \n",
    "              'two', 'u', 'un', 'under', 'unfortunately', 'unless', 'unlikely', 'until', 'unto', 'up', 'upon', 'us',\n",
    "              'use', 'used', 'useful', 'uses', 'using', 'usually', 'uucp', 'v', 'value', 'various', 'very', 'via',\n",
    "              'viz', 'vs', 'w', 'want', 'wants', 'was', 'way', 'we', 'welcome', 'well', 'went', 'were', 'what', \n",
    "              'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein',\n",
    "              'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom',\n",
    "              'whose', 'why', 'will', 'willing', 'wish', 'with', 'within', 'without', 'wonder', 'would', 'x', 'y',\n",
    "              'yes', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves', 'z','anything', 'should']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c88a8b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given token_pattern parameter is applied in extract_ngrams function to normalize all text.\n",
    "\n",
    "def extract_ngrams(x_raw, n = (1), stop_words=stop_words, vocab = set(),\n",
    "                   token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b'):\n",
    "    \n",
    "    tokenRE = re.compile(token_pattern)\n",
    "    x_raw_unigrams = [w for w in tokenRE.findall(str(x_raw).lower(),) if w not in stop_words]\n",
    "    ngrams = []\n",
    "    for num in n:\n",
    "        ngrams+= [x_raw_unigrams[i:i+num] for i in range(len(x_raw_unigrams)-num+1)]                \n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf49f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is just to see the all ngrams clear and make them more readable\n",
    "\n",
    "def modify_ngrams(docs):\n",
    "    modified_ngrams = []\n",
    "    for k in range(len(docs)):\n",
    "        ngrams = np.array([' '.join([str(ngram) for ngram in doc]) for doc in docs[k]])\n",
    "        modified_ngrams.append(ngrams)\n",
    "    return modified_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "277df8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngrams are computed before the get_vocab function\n",
    "# it is more efficient because otherwise training data will be computed twice:\n",
    "# 1st: inside the get_vocab and 2nd: with extract_ngrams function\n",
    "\n",
    "ngrams_train = modify_ngrams([extract_ngrams(train_list_x[i], n =[1])\n",
    "                              for i in range(len(train_list_x))])\n",
    "ngrams_dev = modify_ngrams([extract_ngrams(dev_list_x[i], n =[1])\n",
    "                            for i in range(len(dev_list_x))])\n",
    "ngrams_test = modify_ngrams([extract_ngrams(test_list_x[i], n =[1])\n",
    "                             for i in range(len(test_list_x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0a88c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab size is selected as 2000 considering the total amount of data and the computation cost. If vocab size is so large,\n",
    "# model performance can be slow and expensive, also in may cause overfitting. \n",
    "# If it is less than enough, the model can not achieve desired success.\n",
    "# there is also another criteria as min_df, which is selected as 4,\n",
    "# to eleminate ngrams which are not common in all documents. \n",
    "\n",
    "def get_vocab(X_raw, min_df= 4, keep_topN=2000):\n",
    "    df=Counter()\n",
    "    for ngram in X_raw:\n",
    "        df.update(set(ngram))\n",
    "    df = Counter({k: c for k, c in df.items() if c >= min_df})\n",
    "    c = Counter(np.concatenate(X_raw))\n",
    "    ngram_counts = dict(Counter({k: c for k, c in c.items() if k in df.keys()}).most_common(keep_topN))\n",
    "    df = Counter({k: c for k, c in df.items() if k in ngram_counts.keys()})\n",
    "    df = dict(sorted(df.items(), key=lambda pair: pair[0], reverse=False))\n",
    "    vocab = set(df)   \n",
    "    return  vocab, df, ngram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cba411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, df, ngram_counts = get_vocab(ngrams_train)\n",
    "id_vocab = dict(enumerate((sorted(vocab))))\n",
    "vocab_id = {val:key for key,val in id_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f17983f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-84598dc30852>:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train_indices_init = np.array([np.array([vocab_id[word] for word in words]) for words in ngrams_train])\n",
      "<ipython-input-10-84598dc30852>:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  dev_indices_init = np.array([np.array([vocab_id[word] for word in words]) for words in ngrams_dev])\n",
      "<ipython-input-10-84598dc30852>:19: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  test_indices_init = np.array([np.array([vocab_id[word] for word in words]) for words in ngrams_test])\n"
     ]
    }
   ],
   "source": [
    "# conditions in the get_vocab functions (ngram is in the vocab or not) are checked here. \n",
    "ngrams_train =[[n for n in t if n in vocab] for t in ngrams_train]\n",
    "ngrams_dev = [[n for n in t if n in vocab] for t in ngrams_dev]\n",
    "ngrams_test = [[n for n in t if n in vocab] for t in ngrams_test]\n",
    "\n",
    "# first, initial indices are computed for all data\n",
    "# secondly, final indices are created removing the documents(examples) with no indices(unigrams) as per the vocab\n",
    "\n",
    "train_indices_init = np.array([np.array([vocab_id[word] for word in words]) for words in ngrams_train])\n",
    "train_zero_id = dict(enumerate(train_indices_init, 0))\n",
    "train_zero_list = [k for k,v in train_zero_id.items() if len(v) == 0]\n",
    "train_indices_final =np.delete(train_indices_init, train_zero_list)\n",
    "\n",
    "dev_indices_init = np.array([np.array([vocab_id[word] for word in words]) for words in ngrams_dev])  \n",
    "dev_zero_id = dict(enumerate(dev_indices_init, 0))\n",
    "dev_zero_list = [k for k,v in dev_zero_id.items() if len(v) == 0]\n",
    "dev_indices_final =np.delete(dev_indices_init, dev_zero_list)\n",
    "\n",
    "test_indices_init = np.array([np.array([vocab_id[word] for word in words]) for words in ngrams_test]) \n",
    "test_zero_id = dict(enumerate(test_indices_init, 0))\n",
    "test_zero_list = [k for k,v in test_zero_id.items() if len(v) == 0]\n",
    "test_indices_final =np.delete(test_indices_init, test_zero_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0233a349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty vectors are deleted \n",
    "train_arr_y = np.delete(df_train['label'].to_numpy(), train_zero_list)\n",
    "dev_arr_y = np.delete(df_dev['label'].to_numpy(), dev_zero_list)\n",
    "test_arr_y = np.delete(df_test['label'].to_numpy(), test_zero_list)\n",
    "\n",
    "# label arrays are converted one-hot vector to compute loss function and derivatives of output\n",
    "train_arr_y_onehot = np.array([np.array( [np.eye(3)[i-1]]) for i in train_arr_y])\n",
    "dev_arr_y_onehot = np.array([np.array( [np.eye(3)[i-1]]) for i in dev_arr_y])\n",
    "test_arr_y_onehot = np.array([np.array( [np.eye(3)[i-1]]) for i in test_arr_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a053038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_weights(vocab_size= len(vocab), embedding_dim = 4, \n",
    "                    hidden_dim=[1], num_classes=3, init_val = 0.1):\n",
    "\n",
    "    W={}\n",
    "    matrix_dim_list = []\n",
    "    matrix_dim_list.append(vocab_size)\n",
    "    matrix_dim_list.append(embedding_dim)\n",
    "    matrix_dim_list.extend(hidden_dim)\n",
    "    matrix_dim_list.append(num_classes)\n",
    "        \n",
    "    for i in range(len(matrix_dim_list)-1):\n",
    "        np.random.seed(123)\n",
    "        W[i] = np.random.uniform(-init_val, init_val, (matrix_dim_list[i], matrix_dim_list[i+1])).astype('float32')\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c471959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    k = 0.0000001  # to prevent undefined log(0) issue, very small number (k) are added into the log()  \n",
    "    z = z.ravel()\n",
    "    e = (np.exp(z - np.max(z)))\n",
    "    softmax = (e / e.sum(axis=0)+k)\n",
    "    return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "340326c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_loss(y, y_preds):\n",
    "    k = 0.0000001  # to prevent undefined log(0) issue, very small number (k) are added into the log()    \n",
    "    l = -np.sum(y * np.log(y_preds + k))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10131ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    z_copy = z.copy()\n",
    "    a = np.maximum(0.0, z_copy)\n",
    "    return a\n",
    "\n",
    "def relu_derivative(z):\n",
    "    z_copy = z.copy() \n",
    "    dz = np.greater(z_copy, 0.).astype(np.float32)\n",
    "    return dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07b58988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_mask(size, dropout_rate): \n",
    "    ratio = int(dropout_rate * size)\n",
    "    dropout_vec = np.ones(size)\n",
    "    dropout_vec[:ratio] = 0\n",
    "    np.random.seed(123)\n",
    "    np.random.shuffle(dropout_vec)\n",
    "    return dropout_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5664d059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(x, W, dropout_rate=0.2):\n",
    "    out_vals = {}\n",
    "    h_vecs = []\n",
    "    a_vecs = []\n",
    "    drop_mask_vecs = []\n",
    "    for k in range(len(W)-1):\n",
    "        if k == 0:\n",
    "            \n",
    "            # the embedding matrix is computed as per the given network structure\n",
    "            # then the matrix is converted to the first hidden layer\n",
    "            matrix = W[k][x]\n",
    "            h = np.nan_to_num((np.sum(matrix, axis = 0) / len(x)).reshape(1, -1).astype('float32'))            \n",
    "        else:\n",
    "            h = np.nan_to_num(np.dot(layer_output, W[k]).astype('float32')) \n",
    "        \n",
    "        # relu function is applied\n",
    "        a = np.nan_to_num(relu(h).astype('float32'))\n",
    "        \n",
    "        # some part of layer is removed to apply regularisation \n",
    "        vector_drop = np.nan_to_num(dropout_mask(a.shape[1], dropout_rate).reshape(1, -1).astype('float32'))\n",
    "        layer_output = np.nan_to_num(a) * np.nan_to_num(vector_drop)     \n",
    "        \n",
    "        h_vecs.append(h)\n",
    "        a_vecs.append(a)\n",
    "        drop_mask_vecs.append(vector_drop)\n",
    "    z = np.nan_to_num(np.dot(layer_output, W[len(W)-1]).astype('float32'))\n",
    "    \n",
    "    # using softmax activation function to get required probabilities \n",
    "    prediction = softmax(z)    \n",
    "    prediction = prediction.reshape(1, -1)       \n",
    "    out_vals['h'] = h_vecs\n",
    "    out_vals['a'] = a_vecs\n",
    "    out_vals['drop_mask_vecs'] = drop_mask_vecs\n",
    "    out_vals['prediction'] = prediction      \n",
    "    return out_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e0e8522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(x, y, W, out_vals, lr=0.001, freeze_emb=False):\n",
    "    for i in range(len(W)-1):\n",
    "        \n",
    "        # first part of condition statement is for computation btw the output layer \n",
    "        # and the last hidden layer which is before output layer \n",
    "        if i == 0:           \n",
    "            y_pred = np.nan_to_num(out_vals['prediction']).astype('float32')\n",
    "            z = np.nan_to_num(out_vals['a'][-1] * out_vals['drop_mask_vecs'][-1]).astype('float32')\n",
    "            dl_dz = np.nan_to_num(y_pred - y).astype('float32')\n",
    "            dl_dw = np.nan_to_num(z.T * dl_dz).astype('float32')\n",
    "            dl_dz = np.nan_to_num(np.dot(dl_dz, W[len(W)-1].T)).astype('float32')            \n",
    "            W[len(W)-1] = W[len(W)-1] - lr * dl_dw\n",
    "        \n",
    "        # second part of condition statement is for computation btw the last hidden layer which is before output layer \n",
    "        # and following hidden layers. \n",
    "        else:            \n",
    "            dl_dz = dl_dz  * np.nan_to_num(relu_derivative(out_vals['h'][len(W)-1-i])).astype('float32')           \n",
    "            z = np.nan_to_num(out_vals['a'][len(W)-2-i] * out_vals['drop_mask_vecs'][len(W)-2-i]).astype('float32')\n",
    "            dl_dw = np.nan_to_num(np.dot(z.T, dl_dz)).astype('float32')         \n",
    "            dl_dz = np.nan_to_num(np.dot(dl_dz, W[len(W)-1-i].T)).astype('float32')           \n",
    "            W[len(W)-1-i] =np.nan_to_num( W[len(W)-1-i] - lr * dl_dw).astype('float32')    \n",
    "    \n",
    "    # here, the condition below is for updating of embedding matrix. \n",
    "    # It is assumed that all inputs (X) are 1 and weight matrix is the dot product of inputs and weights.\n",
    "    # So, embedding matrix is also weights matrix in this case.\n",
    "    \n",
    "    if not freeze_emb:        \n",
    "        unique_index = list(set(x))\n",
    "        unique_index.sort()\n",
    "        input_layer = np.ones(len(unique_index)).reshape(1, -1)\n",
    "        dw = np.dot(input_layer.T, dl_dz)        \n",
    "        W[0][unique_index] = W[0][unique_index] - lr * dw        \n",
    "    return W    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbe08f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(X_tr, Y_tr, W, X_dev, Y_dev, lr, \n",
    "        dropout_rate, epochs, tolerance, freeze_emb, \n",
    "        print_progress=True):    \n",
    "    training_loss_history =  np.zeros(0)\n",
    "    validation_loss_history =  np.zeros(0)    \n",
    "    for i in range(epochs):\n",
    "        rand1 = np.random.permutation(len(X_tr))\n",
    "        X_tr = X_tr[rand1]\n",
    "        Y_tr = Y_tr[rand1]        \n",
    "        train_initial_loss = np.zeros(0)\n",
    "        val_initial_loss = np.array([1,10])\n",
    "        \n",
    "        for k in range(len(X_tr)):            \n",
    "            layer_outputs_train = forward_pass(X_tr[k], W, dropout_rate)\n",
    "            W = backward_pass(X_tr[k], Y_tr[k], W, layer_outputs_train, lr, freeze_emb)\n",
    "            \n",
    "            # here, layer outputs computed again to get the final loss results after updating weights.\n",
    "            layer_outputs_train_updated = forward_pass(X_tr[k], W, dropout_rate)\n",
    "            \n",
    "            loss_train = categorical_loss(Y_tr[k], layer_outputs_train['prediction'])\n",
    "            train_initial_loss = np.append(train_initial_loss, loss_train)            \n",
    "        training_loss_history = np.append(training_loss_history, train_initial_loss.mean())\n",
    "        for j in range(len(X_dev)):            \n",
    "            layer_outputs_val = forward_pass(X_dev[j], W, dropout_rate)\n",
    "            loss_dev = categorical_loss(Y_dev[j], layer_outputs_val['prediction'])\n",
    "            val_initial_loss = np.append(val_initial_loss, loss_dev)\n",
    "            diff = np.absolute(val_initial_loss[-1] - val_initial_loss[-2])\n",
    "            if diff < tolerance:\n",
    "                break                \n",
    "        validation_loss_history = np.append(validation_loss_history, (val_initial_loss[2:]).mean())        \n",
    "        if print_progress:\n",
    "            print(f\"epoch:{i+1}  \" , f\"training_loss: {training_loss_history[-1]}  \", \n",
    "                  f\"validation_loss: {validation_loss_history[-1]} \")        \n",
    "    return W, training_loss_history, validation_loss_history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14a01604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the grid search method is used to decide the hyperparameters combination\n",
    "# for embedding_dim,  dropout_rate (regularisation) and lr using hyper_param function.\n",
    "# first, the loss function will be calculated for all hyperparameters combinations\n",
    "# then, the hyperparameters which give the lowest Loss will be selected. \n",
    "# the \"comparison\" table also can be seen to see the effects of different hyperparameters on the performance. \n",
    "# then, SGD function will be computed with hyperparameters obtained from hyper_param function to evaluate scores\n",
    "# total epoch number is the same for all model implementation\n",
    "\n",
    "num = 3\n",
    "dropout_rate_vec = np.array([0.5, 0.7, 0.8])\n",
    "lr_vec = np.array([0.01, 0.001, 0.0001])\n",
    "embedding_dim_vec = np.array([200, 250, 300])\n",
    "loss_matrix_train = np.empty([num, num, num])\n",
    "loss_matrix_val = np.empty([num, num, num])\n",
    "\n",
    "\n",
    "def hyper_params(train_indices_final, train_arr_y_onehot,\n",
    "                            X_dev=dev_indices_final, \n",
    "                            Y_dev=dev_arr_y_onehot,\n",
    "                            hidden_dim=[],\n",
    "                            freeze_emb=False,\n",
    "                            tolerance=0.001,\n",
    "                            epochs=5):   \n",
    "    accuracy_train =  []\n",
    "    accuracy_val =  []\n",
    "    for i in range(num):\n",
    "        for j in range(num):\n",
    "            for k in range(num):                 \n",
    "                W = network_weights(vocab_size=len(vocab),embedding_dim=embedding_dim_vec[k],\n",
    "                hidden_dim=hidden_dim, num_classes=3)\n",
    "                dropout_rate = dropout_rate_vec[i]\n",
    "                lr = lr_vec[j]\n",
    "                weights, training_loss_history, validation_loss_history = SGD(train_indices_final, train_arr_y_onehot,\n",
    "                                W,\n",
    "                                X_dev=dev_indices_final, \n",
    "                                Y_dev=dev_arr_y_onehot,\n",
    "                                lr=lr, \n",
    "                                dropout_rate=dropout_rate,\n",
    "                                freeze_emb=freeze_emb,\n",
    "                                tolerance=tolerance,\n",
    "                                epochs=epochs,\n",
    "                                print_progress=False)\n",
    "                \n",
    "                preds_train = np.array([np.argmax(forward_pass(x, weights, dropout_rate=0.0)['prediction'])+1\n",
    "            for x in train_indices_final])\n",
    "                accuracy = accuracy_score(train_arr_y,preds_train)\n",
    "                accuracy_train.append(accuracy)\n",
    "                \n",
    "                preds_val = np.array([np.argmax(forward_pass(x, weights, dropout_rate=0.0)['prediction'])+1\n",
    "            for x in dev_indices_final])\n",
    "                accuracy = accuracy_score(dev_arr_y,preds_val)\n",
    "                accuracy_val.append(accuracy)\n",
    "                \n",
    "                \n",
    "                loss_matrix_train[i, j, k] = training_loss_history.min()\n",
    "                loss_matrix_val[i, j, k] = validation_loss_history.min()               \n",
    "    best_params = np.argwhere(loss_matrix_train == np.min(loss_matrix_train))\n",
    "    dropout_rate = dropout_rate_vec[best_params[0][0]]\n",
    "    lr   = lr_vec[best_params[0][1]]\n",
    "    embedding_dim = embedding_dim_vec[best_params[0][2]]    \n",
    "    \n",
    "    index = pd.MultiIndex.from_product([dropout_rate_vec, lr_vec, embedding_dim_vec])  \n",
    "    comparison = (pd.DataFrame(loss_matrix_train.flatten(), index=index, columns=['train loss']))\\\n",
    "        .rename_axis(['dropout rate', 'learning rate', 'embedding dim' ])\n",
    "    comparison['train accuracy'] = accuracy_train\n",
    "    comparison['validation loss'] = loss_matrix_val.flatten()\n",
    "    comparison['validation accuracy'] = accuracy_val\n",
    "            \n",
    "    W = network_weights(vocab_size=len(vocab),embedding_dim=embedding_dim,\n",
    "                hidden_dim=hidden_dim, num_classes=3)\n",
    "    return W, dropout_rate, lr, embedding_dim, comparison\n",
    "\n",
    "# this hyperparameter function is for the case that glove weights are used\n",
    "# and that is why embedding dimension is fix as 300\n",
    "loss_matrix_train_glove = np.empty([num, num])\n",
    "loss_matrix_val_glove = np.empty([num, num])\n",
    "\n",
    "def hyper_params_glove(train_indices_final, train_arr_y_onehot,\n",
    "                            X_dev=dev_indices_final, \n",
    "                            Y_dev=dev_arr_y_onehot,\n",
    "                            hidden_dim=[],\n",
    "                            freeze_emb=True,\n",
    "                            tolerance=0.001,\n",
    "                            epochs=5):\n",
    "    accuracy_train_glove =[]\n",
    "    accuracy_val_glove =[]\n",
    "    for i in range(num):\n",
    "        for j in range(num):\n",
    "            embedding_dim = 300    \n",
    "            W = network_weights(vocab_size=len(vocab),embedding_dim=embedding_dim,\n",
    "            hidden_dim=hidden_dim, num_classes=3)\n",
    "            W[0] = w_glove            \n",
    "            dropout_rate = dropout_rate_vec[i]\n",
    "            lr = lr_vec[j]\n",
    "            weights, training_loss_history, validation_loss_history = SGD(train_indices_final, train_arr_y_onehot,\n",
    "                            W,\n",
    "                            X_dev=dev_indices_final, \n",
    "                            Y_dev=dev_arr_y_onehot,\n",
    "                            lr=lr, \n",
    "                            dropout_rate=dropout_rate,\n",
    "                            freeze_emb=freeze_emb,\n",
    "                            tolerance=tolerance,\n",
    "                            epochs=epochs,\n",
    "                            print_progress=False)\n",
    "            \n",
    "            loss_matrix_train_glove[i, j] = training_loss_history.min()\n",
    "            loss_matrix_val_glove[i, j] = validation_loss_history.min()  \n",
    "            \n",
    "            preds_train = np.array([np.argmax(forward_pass(x, weights, dropout_rate=0.0)['prediction'])+1 \n",
    "                                    for x in train_indices_final])\n",
    "            accuracy = accuracy_score(train_arr_y, preds_train)\n",
    "            accuracy_train_glove.append(accuracy)\n",
    "                \n",
    "            preds_val = np.array([np.argmax(forward_pass(x, weights, dropout_rate=0.0)['prediction'])+1 \n",
    "                                  for x in dev_indices_final])\n",
    "            accuracy = accuracy_score(dev_arr_y, preds_val)\n",
    "            accuracy_val_glove.append(accuracy)\n",
    "                \n",
    "    best_params = np.argwhere(loss_matrix_train_glove == np.min(loss_matrix_train_glove))\n",
    "    dropout_rate = dropout_rate_vec[best_params[0][0]]\n",
    "    lr   = lr_vec[best_params[0][1]]    \n",
    "    \n",
    "    index = pd.MultiIndex.from_product([dropout_rate_vec, lr_vec])\n",
    "    comparison = (pd.DataFrame(loss_matrix_train_glove.flatten(), index=index, columns=['train loss']))\\\n",
    "        .rename_axis(['dropout rate', 'learning rate'])\n",
    "\n",
    "    comparison['train accuracy'] = accuracy_train_glove\n",
    "    comparison['validation loss'] = loss_matrix_val_glove.flatten()\n",
    "    comparison['validation accuracy'] = accuracy_val_glove\n",
    "    \n",
    "    return W, dropout_rate, lr, embedding_dim, comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32eb357",
   "metadata": {},
   "source": [
    "### Average Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4674d835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape W0 (2000, 300)\n",
      "Shape W1 (300, 3)\n",
      "best hiper parameters: ('embedding_dim', 300) ('dropout_rate', 0.5) ('learning rate', 0.01)\n"
     ]
    }
   ],
   "source": [
    "W1, dropout_rate, lr, embedding_dim, comparison = hyper_params(train_indices_final, train_arr_y_onehot,\n",
    "                            X_dev=dev_indices_final, \n",
    "                            Y_dev=dev_arr_y_onehot,\n",
    "                            hidden_dim=[],\n",
    "                            freeze_emb=False,\n",
    "                            tolerance=0.001,\n",
    "                            epochs=5)\n",
    "for i in range(len(W1)):\n",
    "    print('Shape W'+str(i), W1[i].shape)\n",
    "    \n",
    "print(('best hiper parameters:'),('embedding_dim', embedding_dim), \n",
    "      ('dropout_rate', dropout_rate), \n",
    "      ('learning rate' ,lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7456ae2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train loss</th>\n",
       "      <th>train accuracy</th>\n",
       "      <th>validation loss</th>\n",
       "      <th>validation accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropout rate</th>\n",
       "      <th>learning rate</th>\n",
       "      <th>embedding dim</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">0.5</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">0.0100</th>\n",
       "      <th>200</th>\n",
       "      <td>0.449486</td>\n",
       "      <td>0.915833</td>\n",
       "      <td>0.487788</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0.422741</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.465409</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.414351</td>\n",
       "      <td>0.917500</td>\n",
       "      <td>0.469334</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">0.0010</th>\n",
       "      <th>200</th>\n",
       "      <td>1.087165</td>\n",
       "      <td>0.799167</td>\n",
       "      <td>1.095017</td>\n",
       "      <td>0.693333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>1.085886</td>\n",
       "      <td>0.796667</td>\n",
       "      <td>1.083525</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>1.084328</td>\n",
       "      <td>0.822917</td>\n",
       "      <td>1.085890</td>\n",
       "      <td>0.713333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">0.0001</th>\n",
       "      <th>200</th>\n",
       "      <td>1.096865</td>\n",
       "      <td>0.465417</td>\n",
       "      <td>1.100314</td>\n",
       "      <td>0.453333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>1.097380</td>\n",
       "      <td>0.481667</td>\n",
       "      <td>1.087531</td>\n",
       "      <td>0.393333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>1.097776</td>\n",
       "      <td>0.427500</td>\n",
       "      <td>1.094637</td>\n",
       "      <td>0.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">0.7</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">0.0100</th>\n",
       "      <th>200</th>\n",
       "      <td>0.515907</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>0.536517</td>\n",
       "      <td>0.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0.455589</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>0.488595</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.460949</td>\n",
       "      <td>0.915417</td>\n",
       "      <td>0.455196</td>\n",
       "      <td>0.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">0.0010</th>\n",
       "      <th>200</th>\n",
       "      <td>1.091298</td>\n",
       "      <td>0.797083</td>\n",
       "      <td>1.097672</td>\n",
       "      <td>0.713333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>1.089746</td>\n",
       "      <td>0.795000</td>\n",
       "      <td>1.090161</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>1.090689</td>\n",
       "      <td>0.824167</td>\n",
       "      <td>1.090192</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">0.0001</th>\n",
       "      <th>200</th>\n",
       "      <td>1.097458</td>\n",
       "      <td>0.464167</td>\n",
       "      <td>1.102848</td>\n",
       "      <td>0.453333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>1.097484</td>\n",
       "      <td>0.481250</td>\n",
       "      <td>1.090602</td>\n",
       "      <td>0.393333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>1.098183</td>\n",
       "      <td>0.424583</td>\n",
       "      <td>1.094536</td>\n",
       "      <td>0.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">0.8</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">0.0100</th>\n",
       "      <th>200</th>\n",
       "      <td>0.549808</td>\n",
       "      <td>0.909583</td>\n",
       "      <td>0.561597</td>\n",
       "      <td>0.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0.505235</td>\n",
       "      <td>0.914167</td>\n",
       "      <td>0.524269</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.508729</td>\n",
       "      <td>0.914167</td>\n",
       "      <td>0.530801</td>\n",
       "      <td>0.873333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">0.0010</th>\n",
       "      <th>200</th>\n",
       "      <td>1.094017</td>\n",
       "      <td>0.795000</td>\n",
       "      <td>1.101208</td>\n",
       "      <td>0.713333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>1.092292</td>\n",
       "      <td>0.795000</td>\n",
       "      <td>1.096494</td>\n",
       "      <td>0.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>1.093747</td>\n",
       "      <td>0.822917</td>\n",
       "      <td>1.096604</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">0.0001</th>\n",
       "      <th>200</th>\n",
       "      <td>1.097798</td>\n",
       "      <td>0.463333</td>\n",
       "      <td>1.101719</td>\n",
       "      <td>0.453333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>1.097849</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>1.096694</td>\n",
       "      <td>0.393333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>1.098488</td>\n",
       "      <td>0.423750</td>\n",
       "      <td>1.097787</td>\n",
       "      <td>0.340000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          train loss  train accuracy  \\\n",
       "dropout rate learning rate embedding dim                               \n",
       "0.5          0.0100        200              0.449486        0.915833   \n",
       "                           250              0.422741        0.916667   \n",
       "                           300              0.414351        0.917500   \n",
       "             0.0010        200              1.087165        0.799167   \n",
       "                           250              1.085886        0.796667   \n",
       "                           300              1.084328        0.822917   \n",
       "             0.0001        200              1.096865        0.465417   \n",
       "                           250              1.097380        0.481667   \n",
       "                           300              1.097776        0.427500   \n",
       "0.7          0.0100        200              0.515907        0.912500   \n",
       "                           250              0.455589        0.915000   \n",
       "                           300              0.460949        0.915417   \n",
       "             0.0010        200              1.091298        0.797083   \n",
       "                           250              1.089746        0.795000   \n",
       "                           300              1.090689        0.824167   \n",
       "             0.0001        200              1.097458        0.464167   \n",
       "                           250              1.097484        0.481250   \n",
       "                           300              1.098183        0.424583   \n",
       "0.8          0.0100        200              0.549808        0.909583   \n",
       "                           250              0.505235        0.914167   \n",
       "                           300              0.508729        0.914167   \n",
       "             0.0010        200              1.094017        0.795000   \n",
       "                           250              1.092292        0.795000   \n",
       "                           300              1.093747        0.822917   \n",
       "             0.0001        200              1.097798        0.463333   \n",
       "                           250              1.097849        0.480000   \n",
       "                           300              1.098488        0.423750   \n",
       "\n",
       "                                          validation loss  validation accuracy  \n",
       "dropout rate learning rate embedding dim                                        \n",
       "0.5          0.0100        200                   0.487788             0.866667  \n",
       "                           250                   0.465409             0.866667  \n",
       "                           300                   0.469334             0.866667  \n",
       "             0.0010        200                   1.095017             0.693333  \n",
       "                           250                   1.083525             0.733333  \n",
       "                           300                   1.085890             0.713333  \n",
       "             0.0001        200                   1.100314             0.453333  \n",
       "                           250                   1.087531             0.393333  \n",
       "                           300                   1.094637             0.340000  \n",
       "0.7          0.0100        200                   0.536517             0.860000  \n",
       "                           250                   0.488595             0.866667  \n",
       "                           300                   0.455196             0.860000  \n",
       "             0.0010        200                   1.097672             0.713333  \n",
       "                           250                   1.090161             0.733333  \n",
       "                           300                   1.090192             0.720000  \n",
       "             0.0001        200                   1.102848             0.453333  \n",
       "                           250                   1.090602             0.393333  \n",
       "                           300                   1.094536             0.340000  \n",
       "0.8          0.0100        200                   0.561597             0.860000  \n",
       "                           250                   0.524269             0.866667  \n",
       "                           300                   0.530801             0.873333  \n",
       "             0.0010        200                   1.101208             0.713333  \n",
       "                           250                   1.096494             0.740000  \n",
       "                           300                   1.096604             0.720000  \n",
       "             0.0001        200                   1.101719             0.453333  \n",
       "                           250                   1.096694             0.393333  \n",
       "                           300                   1.097787             0.340000  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameter combinations with their loss and accuracy  \n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc4679df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1   training_loss: 1.0812936486303806   validation_loss: 1.0725040993791946 \n",
      "epoch:2   training_loss: 0.9909249879978597   validation_loss: 0.9636095384756724 \n",
      "epoch:3   training_loss: 0.7751730046797699   validation_loss: 0.8365765107529504 \n",
      "epoch:4   training_loss: 0.5575398141176751   validation_loss: 0.5715984737873078 \n",
      "epoch:5   training_loss: 0.4143508175681927   validation_loss: 0.46933378831667155 \n",
      "epoch:6   training_loss: 0.33229522650582416   validation_loss: 0.40016831367189976 \n",
      "epoch:7   training_loss: 0.27943932230619645   validation_loss: 0.38377181681649136 \n",
      "epoch:8   training_loss: 0.24000386498019927   validation_loss: 0.3618921332363971 \n",
      "epoch:9   training_loss: 0.2109537619521979   validation_loss: 0.34658326308980275 \n",
      "epoch:10   training_loss: 0.18775868144726096   validation_loss: 0.2580315166211221 \n",
      "epoch:11   training_loss: 0.167069723937254   validation_loss: 0.28100358048227225 \n",
      "epoch:12   training_loss: 0.149523910000806   validation_loss: 0.30873660608631326 \n",
      "epoch:13   training_loss: 0.13453737519678233   validation_loss: 0.27764485866846017 \n",
      "epoch:14   training_loss: 0.1226813587201256   validation_loss: 0.2834869820481627 \n",
      "epoch:15   training_loss: 0.11103451955389801   validation_loss: 0.2909437335567074 \n",
      "epoch:16   training_loss: 0.10173608979292076   validation_loss: 0.2887721965156301 \n",
      "epoch:17   training_loss: 0.09214296155316912   validation_loss: 0.2887174829626216 \n",
      "epoch:18   training_loss: 0.08430537149011856   validation_loss: 0.2867766482275427 \n",
      "epoch:19   training_loss: 0.07732525887585955   validation_loss: 0.28741505702981995 \n",
      "epoch:20   training_loss: 0.07055120496413765   validation_loss: 0.21583346653563817 \n"
     ]
    }
   ],
   "source": [
    "W1, loss_tr1, dev_loss1 = SGD(train_indices_final, train_arr_y_onehot,\n",
    "                            W1,\n",
    "                            X_dev=dev_indices_final, \n",
    "                            Y_dev=dev_arr_y_onehot,\n",
    "                            lr=lr, \n",
    "                            dropout_rate=dropout_rate,\n",
    "                            freeze_emb=False,\n",
    "                            tolerance=0.001,\n",
    "                            epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb0ad56d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_tr1, label = \"training_loss_history\")\n",
    "plt.plot(dev_loss1, label = \"validation_loss_history\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ed6ef",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "The model is OK with the selected the best hyperparameters. There is no any overfitting and/or underfitting problems. Training Loss are decreasing by every epoch. The model's learning process is not so long or short, so it is reasonable and acceptable considering whole data. The validation loss is fluctuating but it does not look an issue considering that this is a deep learning implementation since the scores are high and there is no overfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "214ee266",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8487208008898777\n",
      "Precision: 0.8514313995561199\n",
      "Recall: 0.8487105165366035\n",
      "F1-Score: 0.8487458521279638\n"
     ]
    }
   ],
   "source": [
    "preds_te_average = np.array([np.argmax(forward_pass(x, W1, dropout_rate=0.0)['prediction'])+1\n",
    "            for x,y in zip(test_indices_final, test_arr_y)])\n",
    "\n",
    "print('Accuracy:', accuracy_score(test_arr_y,preds_te_average))\n",
    "print('Precision:', precision_score(test_arr_y,preds_te_average,average='macro'))\n",
    "print('Recall:', recall_score(test_arr_y,preds_te_average,average='macro'))\n",
    "print('F1-Score:', f1_score(test_arr_y,preds_te_average,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2724df27",
   "metadata": {},
   "source": [
    "There are 27 different hyperparameter combinations (3 for each) checked in this model.\n",
    "Embedding dimension, dropout rate and learning rate are considered as hyperparameters in this implementation. Grid search which is the simplest algorithm for hyperparameter tuning is applied to choose the best hyperparameters. Every combination of values of this grid is tried to find the best combination. The grid values are estimated manually (mostly taking the traditional values). To train the model efficiently, the dropout rate is set as a big range of percentage. This is to prevent the model from overfitting by regularization within the 20 epochs. Also, it can improve the generalization performance on test (unseen) data.\n",
    "Embedding dimension is selected with the numbers up to 300 in traditional ways. To make the dimension so high increases the cost and also the benefit of the high dimension will be limited after a point.\n",
    "Since the given learning rate is good enough to get desired scores, the number of epochs is limited by 20.\n",
    "If the learning rates are small, it requires more epochs to make changes after each iteration/update. Also, the number of epochs (20) is the same for all model implementations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "173a924a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_embeddings(f_zip, f_txt, vocab_id, emb_size=300):\n",
    "    w_emb = np.zeros((len(vocab_id), emb_size))    \n",
    "    with zipfile.ZipFile(f_zip) as z:\n",
    "        with z.open(f_txt) as f:\n",
    "            for line in f:\n",
    "                line = line.decode('utf-8')\n",
    "                word = line.split(' ')[0]\n",
    "                if word in vocab:\n",
    "                    emb = np.array(line.strip('\\n').split()[1:]).astype(np.float32)\n",
    "                    w_emb[vocab_id[word]] +=emb\n",
    "    return w_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "483dc30a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w_glove = get_glove_embeddings(\"glove.840B.300d.zip\",\"glove.840B.300d.txt\",vocab_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db06130",
   "metadata": {},
   "source": [
    "### Average Embedding (Pre-trained) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebdd0021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape W0 (2000, 300)\n",
      "Shape W1 (300, 3)\n",
      "('embedding_dim', 300) ('dropout_rate', 0.5) ('learning rate', 0.01)\n"
     ]
    }
   ],
   "source": [
    "W2, dropout_rate, lr, embedding_dim, comparison = hyper_params_glove(train_indices_final, train_arr_y_onehot,\n",
    "                            X_dev=dev_indices_final, \n",
    "                            Y_dev=dev_arr_y_onehot,\n",
    "                            hidden_dim=[],\n",
    "                            freeze_emb=True,\n",
    "                            tolerance=0.001,\n",
    "                            epochs=5)\n",
    "for i in range(len(W2)):\n",
    "    print('Shape W'+str(i), W2[i].shape)\n",
    "    \n",
    "print(('embedding_dim', embedding_dim), \n",
    "      ('dropout_rate', dropout_rate), \n",
    "      ('learning rate' ,lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a30a5c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train loss</th>\n",
       "      <th>train accuracy</th>\n",
       "      <th>validation loss</th>\n",
       "      <th>validation accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropout rate</th>\n",
       "      <th>learning rate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">0.5</th>\n",
       "      <th>0.0100</th>\n",
       "      <td>0.441359</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>0.393515</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0010</th>\n",
       "      <td>0.842249</td>\n",
       "      <td>0.845833</td>\n",
       "      <td>0.849701</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0001</th>\n",
       "      <td>1.061291</td>\n",
       "      <td>0.578333</td>\n",
       "      <td>1.054426</td>\n",
       "      <td>0.526667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">0.7</th>\n",
       "      <th>0.0100</th>\n",
       "      <td>0.548073</td>\n",
       "      <td>0.865417</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0010</th>\n",
       "      <td>0.954518</td>\n",
       "      <td>0.768750</td>\n",
       "      <td>0.942615</td>\n",
       "      <td>0.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0001</th>\n",
       "      <td>1.079408</td>\n",
       "      <td>0.396667</td>\n",
       "      <td>1.081086</td>\n",
       "      <td>0.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">0.8</th>\n",
       "      <th>0.0100</th>\n",
       "      <td>0.653128</td>\n",
       "      <td>0.835833</td>\n",
       "      <td>0.633410</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0010</th>\n",
       "      <td>1.002938</td>\n",
       "      <td>0.710833</td>\n",
       "      <td>0.992430</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0001</th>\n",
       "      <td>1.087198</td>\n",
       "      <td>0.378750</td>\n",
       "      <td>1.078008</td>\n",
       "      <td>0.433333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            train loss  train accuracy  validation loss  \\\n",
       "dropout rate learning rate                                                \n",
       "0.5          0.0100           0.441359        0.876667         0.393515   \n",
       "             0.0010           0.842249        0.845833         0.849701   \n",
       "             0.0001           1.061291        0.578333         1.054426   \n",
       "0.7          0.0100           0.548073        0.865417         0.570000   \n",
       "             0.0010           0.954518        0.768750         0.942615   \n",
       "             0.0001           1.079408        0.396667         1.081086   \n",
       "0.8          0.0100           0.653128        0.835833         0.633410   \n",
       "             0.0010           1.002938        0.710833         0.992430   \n",
       "             0.0001           1.087198        0.378750         1.078008   \n",
       "\n",
       "                            validation accuracy  \n",
       "dropout rate learning rate                       \n",
       "0.5          0.0100                    0.880000  \n",
       "             0.0010                    0.866667  \n",
       "             0.0001                    0.526667  \n",
       "0.7          0.0100                    0.866667  \n",
       "             0.0010                    0.740000  \n",
       "             0.0001                    0.440000  \n",
       "0.8          0.0100                    0.833333  \n",
       "             0.0010                    0.700000  \n",
       "             0.0001                    0.433333  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameter combinations with their loss and accuracy  \n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d78c65c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1   training_loss: 0.8432988763973117   validation_loss: 0.6752879132827123 \n",
      "epoch:2   training_loss: 0.6058914583378161   validation_loss: 0.5441248442232609 \n",
      "epoch:3   training_loss: 0.5166545316080252   validation_loss: 0.3995199767606599 \n",
      "epoch:4   training_loss: 0.46965236628156465   validation_loss: 0.44903649238248666 \n",
      "epoch:5   training_loss: 0.4413882156288795   validation_loss: 0.41642243673404056 \n",
      "epoch:6   training_loss: 0.42100747124874033   validation_loss: 0.4002026483540734 \n",
      "epoch:7   training_loss: 0.4060047984707247   validation_loss: 0.32441622419999194 \n",
      "epoch:8   training_loss: 0.39504674864127687   validation_loss: 0.3687897468917072 \n",
      "epoch:9   training_loss: 0.38628265432009357   validation_loss: 0.3206145608654389 \n",
      "epoch:10   training_loss: 0.3781232411729191   validation_loss: 0.3575491914556672 \n",
      "epoch:11   training_loss: 0.372364467302541   validation_loss: 0.2989966841414571 \n",
      "epoch:12   training_loss: 0.3668457051566414   validation_loss: 0.3152587016423543 \n",
      "epoch:13   training_loss: 0.36255802843899193   validation_loss: 0.3318842699114854 \n",
      "epoch:14   training_loss: 0.358903338888825   validation_loss: 0.3051506852416295 \n",
      "epoch:15   training_loss: 0.3545957784436056   validation_loss: 0.3092625022924859 \n",
      "epoch:16   training_loss: 0.35000464957610045   validation_loss: 0.3254100867174566 \n",
      "epoch:17   training_loss: 0.3474475429425729   validation_loss: 0.34259673827327786 \n",
      "epoch:18   training_loss: 0.34551627773738197   validation_loss: 0.33952066679091625 \n",
      "epoch:19   training_loss: 0.34337521845194713   validation_loss: 0.32216754051017193 \n",
      "epoch:20   training_loss: 0.3395861515053912   validation_loss: 0.3229300719841073 \n"
     ]
    }
   ],
   "source": [
    "W2, loss_tr2, dev_loss2 = SGD(train_indices_final, train_arr_y_onehot,\n",
    "                            W2,\n",
    "                            X_dev=dev_indices_final, \n",
    "                            Y_dev=dev_arr_y_onehot,\n",
    "                            lr=lr, \n",
    "                            dropout_rate=dropout_rate,\n",
    "                            freeze_emb=True,\n",
    "                            tolerance=0.001,\n",
    "                            epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "763437b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_tr2, label = \"training_loss_history\")\n",
    "plt.plot(dev_loss2, label = \"validation_loss_history\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6798bc68",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "The model is OK with the selected the best hyperparameters. There is no any overfitting and/or underfitting problems. Training Loss are decreasing by every epoch. The model's learning process is not so long or short, so it is reasonable and acceptable considering whole data. The validation loss is fluctuating but it does not look an issue considering that this is a deep learning implementation since the scores are high and there is no overfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31346fb4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8665183537263627\n",
      "Precision: 0.8692365056095128\n",
      "Recall: 0.8665031586770717\n",
      "F1-Score: 0.8668028093131895\n"
     ]
    }
   ],
   "source": [
    "preds_te_average_glove = np.array([np.argmax(forward_pass(x, W2, dropout_rate=0.0)['prediction'])+1\n",
    "            for x,y in zip(test_indices_final, test_arr_y)])\n",
    "\n",
    "print('Accuracy:', accuracy_score(test_arr_y,preds_te_average_glove))\n",
    "print('Precision:', precision_score(test_arr_y,preds_te_average_glove,average='macro'))\n",
    "print('Recall:', recall_score(test_arr_y,preds_te_average_glove,average='macro'))\n",
    "print('F1-Score:', f1_score(test_arr_y,preds_te_average_glove,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89784f4",
   "metadata": {},
   "source": [
    "The pre-trained model shows better performance than the first model which is trained randomly selected weights. Since GloVe uses global information and also it is able to derive relationship between unigrams(words) from statistics, this is what we expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fff0016",
   "metadata": {},
   "source": [
    "There are 9 different hyperparameter combinations (3 for each) checked in this model.\n",
    "Embedding dimension is fixed as 300 since the glove weights are used.\n",
    "Dropout rate and learning rate are considered as hyperparameters in this implementation. Grid search which is the simplest algorithm for hyperparameter tuning is applied to choose the best hyperparameters. Every combination of values of this grid is tried to find the best combination. The grid values are estimated manually (mostly taking the traditional values). To train the model efficiently, the dropout rate is set as a big range of percentage. This is to prevent the model from overfitting by regularization within the 20 epochs. Also, it can improve the generalization performance on test (unseen) data.\n",
    "Since the given learning rate is good enough to get desired scores, the number of epochs is limited by 20.\n",
    "If the learning rates are small, it requires more epochs to make changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ed96a9",
   "metadata": {},
   "source": [
    "### Average Embedding (Pre-trained) + 1 hidden layers\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa6f3ce",
   "metadata": {},
   "source": [
    "The hidden layer dimension is selected considering the vocabulary size (inputs) and the embedding hidden layer's size (embedding matrix size). Since the vocabulary size is 2000 and embedding layer size is 300 (glove embeddings), the hidden layer size is considered as also 300. Hidden layers with so many neurons can make the model more complex and provide high scores. However, its benefit will be limited after a point. On the other hand, more neurons causes cost and running time issues. In this implementation, as there is no running time issue 300 neurons are selected considering the criterias explained above. In addition to this, because the scores obtained from the model are enough, only 1 hidden layer is considered to decrease cost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8da1cc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape W0 (2000, 300)\n",
      "Shape W1 (300, 300)\n",
      "Shape W2 (300, 3)\n",
      "('embedding_dim', 300) ('dropout_rate', 0.5) ('learning rate', 0.01)\n"
     ]
    }
   ],
   "source": [
    "W3, dropout_rate, lr,embedding_dim, comparison = hyper_params_glove(train_indices_final, train_arr_y_onehot,\n",
    "                            X_dev=dev_indices_final, \n",
    "                            Y_dev=dev_arr_y_onehot,\n",
    "                            hidden_dim=[300],\n",
    "                            freeze_emb=True,\n",
    "                            tolerance=0.001,\n",
    "                            epochs=5)\n",
    "\n",
    "for i in range(len(W3)):\n",
    "    print('Shape W'+str(i), W3[i].shape)\n",
    "    \n",
    "print(('embedding_dim', embedding_dim), \n",
    "      ('dropout_rate', dropout_rate), \n",
    "      ('learning rate' ,lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0b18de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train loss</th>\n",
       "      <th>train accuracy</th>\n",
       "      <th>validation loss</th>\n",
       "      <th>validation accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropout rate</th>\n",
       "      <th>learning rate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">0.5</th>\n",
       "      <th>0.0100</th>\n",
       "      <td>0.317360</td>\n",
       "      <td>0.902917</td>\n",
       "      <td>0.318387</td>\n",
       "      <td>0.873333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0010</th>\n",
       "      <td>0.799459</td>\n",
       "      <td>0.854583</td>\n",
       "      <td>0.805048</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0001</th>\n",
       "      <td>1.073075</td>\n",
       "      <td>0.499167</td>\n",
       "      <td>1.070207</td>\n",
       "      <td>0.546667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">0.7</th>\n",
       "      <th>0.0100</th>\n",
       "      <td>0.361116</td>\n",
       "      <td>0.867500</td>\n",
       "      <td>0.372395</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0010</th>\n",
       "      <td>1.045193</td>\n",
       "      <td>0.625833</td>\n",
       "      <td>1.041617</td>\n",
       "      <td>0.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0001</th>\n",
       "      <td>1.095498</td>\n",
       "      <td>0.357083</td>\n",
       "      <td>1.089302</td>\n",
       "      <td>0.393333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">0.8</th>\n",
       "      <th>0.0100</th>\n",
       "      <td>0.429061</td>\n",
       "      <td>0.847500</td>\n",
       "      <td>0.475645</td>\n",
       "      <td>0.806667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0010</th>\n",
       "      <td>1.078052</td>\n",
       "      <td>0.638333</td>\n",
       "      <td>1.086977</td>\n",
       "      <td>0.653333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0001</th>\n",
       "      <td>1.097592</td>\n",
       "      <td>0.345833</td>\n",
       "      <td>1.093047</td>\n",
       "      <td>0.393333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            train loss  train accuracy  validation loss  \\\n",
       "dropout rate learning rate                                                \n",
       "0.5          0.0100           0.317360        0.902917         0.318387   \n",
       "             0.0010           0.799459        0.854583         0.805048   \n",
       "             0.0001           1.073075        0.499167         1.070207   \n",
       "0.7          0.0100           0.361116        0.867500         0.372395   \n",
       "             0.0010           1.045193        0.625833         1.041617   \n",
       "             0.0001           1.095498        0.357083         1.089302   \n",
       "0.8          0.0100           0.429061        0.847500         0.475645   \n",
       "             0.0010           1.078052        0.638333         1.086977   \n",
       "             0.0001           1.097592        0.345833         1.093047   \n",
       "\n",
       "                            validation accuracy  \n",
       "dropout rate learning rate                       \n",
       "0.5          0.0100                    0.873333  \n",
       "             0.0010                    0.840000  \n",
       "             0.0001                    0.546667  \n",
       "0.7          0.0100                    0.840000  \n",
       "             0.0010                    0.566667  \n",
       "             0.0001                    0.393333  \n",
       "0.8          0.0100                    0.806667  \n",
       "             0.0010                    0.653333  \n",
       "             0.0001                    0.393333  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameter combinations with their loss and accuracy  \n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e998d06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1   training_loss: 0.7745230086923888   validation_loss: 0.4362691030402978 \n",
      "epoch:2   training_loss: 0.40471269843517804   validation_loss: 0.42221437996177347 \n",
      "epoch:3   training_loss: 0.35450747627927437   validation_loss: 0.3336132396615937 \n",
      "epoch:4   training_loss: 0.33249395004226245   validation_loss: 0.36130843326503775 \n",
      "epoch:5   training_loss: 0.3163937927120302   validation_loss: 0.2978215486794824 \n",
      "epoch:6   training_loss: 0.3045736911766714   validation_loss: 0.14345703947668273 \n",
      "epoch:7   training_loss: 0.2922303992371659   validation_loss: 0.2753233848690835 \n",
      "epoch:8   training_loss: 0.2833378994741209   validation_loss: 0.29900015408781416 \n",
      "epoch:9   training_loss: 0.2794564246667111   validation_loss: 0.28593289956601486 \n",
      "epoch:10   training_loss: 0.2691329125945625   validation_loss: 0.2749489961781216 \n",
      "epoch:11   training_loss: 0.262970093192298   validation_loss: 0.2299156928768482 \n",
      "epoch:12   training_loss: 0.25701537796495794   validation_loss: 0.3603220687245437 \n",
      "epoch:13   training_loss: 0.2555530492065964   validation_loss: 0.24867266350004166 \n",
      "epoch:14   training_loss: 0.2500804833618584   validation_loss: 0.2846820195696342 \n",
      "epoch:15   training_loss: 0.24386616820947352   validation_loss: 0.28463560722318376 \n",
      "epoch:16   training_loss: 0.2396769569170207   validation_loss: 0.4105996264968001 \n",
      "epoch:17   training_loss: 0.23840837285409217   validation_loss: 0.4977148133802001 \n",
      "epoch:18   training_loss: 0.2322058767992956   validation_loss: 0.5119713833209479 \n",
      "epoch:19   training_loss: 0.2337224306788221   validation_loss: 0.3531440666190827 \n",
      "epoch:20   training_loss: 0.22257292784666144   validation_loss: 0.2681602336160722 \n"
     ]
    }
   ],
   "source": [
    "W3, loss_tr3, dev_loss3 = SGD(train_indices_final, train_arr_y_onehot,\n",
    "                            W3,\n",
    "                            X_dev=dev_indices_final, \n",
    "                            Y_dev=dev_arr_y_onehot,\n",
    "                            lr=lr, \n",
    "                            dropout_rate=dropout_rate,\n",
    "                            freeze_emb=True,\n",
    "                            tolerance=0.001,\n",
    "                            epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "708265ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_tr3, label = \"training_loss_history\")\n",
    "plt.plot(dev_loss3, label = \"validation_loss_history\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d036eb3b",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "The model is OK with the selected the best hyperparameters. There is no any overfitting and/or underfitting problems. Training Loss are decreasing by every epoch. The model's learning process is not so long or short, so it is reasonable and acceptable considering whole data. The validation loss is fluctuating but it does not look an issue considering that this is a deep learning implementation since the scores are high and there is no overfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4cfbff6d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8765294771968855\n",
      "Precision: 0.8782407003883513\n",
      "Recall: 0.8765031586770716\n",
      "F1-Score: 0.87701146147363\n"
     ]
    }
   ],
   "source": [
    "preds_te_average_glove_hid = np.array([np.argmax(forward_pass(x, W3, dropout_rate=0.0)['prediction'])+1\n",
    "            for x,y in zip(test_indices_final, test_arr_y)])\n",
    "\n",
    "print('Accuracy:', accuracy_score(test_arr_y,preds_te_average_glove_hid))\n",
    "print('Precision:', precision_score(test_arr_y,preds_te_average_glove_hid,average='macro'))\n",
    "print('Recall:', recall_score(test_arr_y,preds_te_average_glove_hid,average='macro'))\n",
    "print('F1-Score:', f1_score(test_arr_y,preds_te_average_glove_hid,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f3e05e",
   "metadata": {},
   "source": [
    "MODEL PERFORMANCE: It can be seen that the deeper architectures increased performance as expected. Since hidden layers make the network more deep, high scores are available. Basically, hidden layers enhance the order of weights, so it is possible to get the best decision boundary. Also, a hidden layer with 300 neurons which is good to have a capable model makes the network able to capture more complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583290e7",
   "metadata": {},
   "source": [
    "HYPER PARAMETERS: There are 9 different hyperparameter combinations (3 for each) checked in this model.\n",
    "Embedding dimension is fixed as 300 since the glove weights are used.\n",
    "Dropout rate and learning rate are considered as hyperparameters in this implementation. Grid search which is the simplest algorithm for hyperparameter tuning is applied to choose the best hyperparameters. Every combination of values of this grid is tried to find the best combination. The grid values are estimated manually (mostly taking the traditional values). To train the model efficiently, the dropout rate is set as a big range of percentage. This is to prevent the model from overfitting by regularization within the 20 epochs. Also, it can improve the generalization performance on test (unseen) data.\n",
    "Since the given learning rate is good enough to get desired scores, the number of epochs is limited by 20.\n",
    "If the learning rates are small, it requires more epochs to make changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c3a2145",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Average Embedding</th>\n",
       "      <td>0.851431</td>\n",
       "      <td>0.848711</td>\n",
       "      <td>0.848746</td>\n",
       "      <td>0.848721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Embedding (Pre-trained)</th>\n",
       "      <td>0.869237</td>\n",
       "      <td>0.866503</td>\n",
       "      <td>0.866803</td>\n",
       "      <td>0.866518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Embedding (Pre-trained) + 1 Hidden Layers</th>\n",
       "      <td>0.878241</td>\n",
       "      <td>0.876503</td>\n",
       "      <td>0.877011</td>\n",
       "      <td>0.876529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Precision    Recall  \\\n",
       "Average Embedding                                   0.851431  0.848711   \n",
       "Average Embedding (Pre-trained)                     0.869237  0.866503   \n",
       "Average Embedding (Pre-trained) + 1 Hidden Layers   0.878241  0.876503   \n",
       "\n",
       "                                                   F1-Score  Accuracy  \n",
       "Average Embedding                                  0.848746  0.848721  \n",
       "Average Embedding (Pre-trained)                    0.866803  0.866518  \n",
       "Average Embedding (Pre-trained) + 1 Hidden Layers  0.877011  0.876529  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'Precision' :[precision_score(test_arr_y,preds_te_average,average='macro'),\n",
    "                      precision_score(test_arr_y,preds_te_average_glove,average='macro'),\n",
    "                      precision_score(test_arr_y,preds_te_average_glove_hid,average='macro')],\n",
    "        'Recall' :[recall_score(test_arr_y,preds_te_average,average='macro'),\n",
    "                   recall_score(test_arr_y,preds_te_average_glove,average='macro'),\n",
    "                   recall_score(test_arr_y,preds_te_average_glove_hid,average='macro')],\n",
    "        'F1-Score' :[f1_score(test_arr_y,preds_te_average,average='macro'),\n",
    "                     f1_score(test_arr_y,preds_te_average_glove,average='macro'),\n",
    "                     f1_score(test_arr_y,preds_te_average_glove_hid,average='macro')],\n",
    "       'Accuracy' : [accuracy_score(test_arr_y,preds_te_average),\n",
    "                     accuracy_score(test_arr_y,preds_te_average_glove),\n",
    "                     accuracy_score(test_arr_y,preds_te_average_glove_hid)]}\n",
    "index = ['Average Embedding', 'Average Embedding (Pre-trained)', 'Average Embedding (Pre-trained) + 1 Hidden Layers']\n",
    "full_results = pd.DataFrame(data, index = index)\n",
    "full_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd33e90",
   "metadata": {},
   "source": [
    "As seen in the final result table above, the best model is \"Average Embedding (Pre-trained) + 1 Hidden Layers\" in terms of all scores. This is what we expected since this model has one more hidden layer with 300 neurons so the neural network is more deep compared to the first two models. Additional hidden layer makes the network more complex. It means there are more relations about the inputs compared to the first 2 models. It looks like there is no big difference between all these 3 models but when the data is huge and/or the result of performance is related to sensitive time requirements and/or cost, the best model will provide more benefit.\n",
    "\n",
    "To sum up, the best model has initialized with the pre-trained glove weights comparing with the first (Average Embedding) model. GloVe uses global information and also it is able to derive relationship between unigrams(words) from statistics. Also the best model has one more hidden layer with 300 neurons (which is well enough to get complexity and more relationship) comparing with the second (Average Embedding (Pre-trained) model. That's why all scores of this model are higher than others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339eb5d5",
   "metadata": {},
   "source": [
    "Note: Whole implementation process (including hyperparameters tuning and running glove function) takes 10 minutes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
